
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Hello!\nMy name is Filip W√≥jcik and I‚Äôm a data scientist and a programmer with specialization in artificial intelligence and machine learning. Recently I have also become a Ph.D. at the Wroclaw University of Economics, IT in Economics Institute, faculty of Business Intelligence in Management. I‚Äôve also graduated as a lawyer but‚Ä¶ well‚Ä¶ everybody makes mistakes, so let‚Äôs skip this part üôÇ\nMachine learning and matters related with data science are my hobby, professional activity and the most important research area. I believe, that I‚Äôve found (so often discussed) balance between those main life areas. Now I try to share my passion with students and all interested people during conferences and lectures. This website was designed to be a platform to share educational materials and my research papers findings.\nApart from that ‚Äì I‚Äôm a happy husband, keeper of two lovely kittens and sports fanatic. In 2023 I became a certified Street Workout and Calisthenics trainer ü§∏. Additionaly I train modern fencing ü§∫ and often go hiking in the mountains.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"09e76bc8aaace0e678c41d60b331ee4d","permalink":"https://maddataanalyst.github.io/en/authors/filip/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/authors/filip/","section":"authors","summary":"Hello!\nMy name is Filip W√≥jcik and I‚Äôm a data scientist and a programmer with specialization in artificial intelligence and machine learning. Recently I have also become a Ph.D. at the Wroclaw University of Economics, IT in Economics Institute, faculty of Business Intelligence in Management.","tags":null,"title":"Filip W√≥jcik","type":"authors"},{"authors":null,"categories":null,"content":" Table of Contents About category Posts in this category Image by storyset on Freepik\nAbout category This category of posts will contain various applications of data science, machine learning, and operations research in finance and economics, like stock trading, portfolio optimization, forecasting, etc.\nPosts in this category Portfolio optimization Portfolio optimization is a critical tool for modern investors willing to maximize their capital utilization in the market and expected returns. Many portfolio optimization techniques are based on different variants of constrained quadratic programming formalizations, taking as inputs expected asset returns, forms of risk metrics (like covariance matrices), and transaction costs.\n","date":1668038400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1668038400,"objectID":"06279256bba1bee34c1e48c0ceb4c700","permalink":"https://maddataanalyst.github.io/en/post/ml-finanace/","publishdate":"2022-11-10T00:00:00Z","relpermalink":"/en/post/ml-finanace/","section":"post","summary":"Section about various financial applications of AI and ML.","tags":null,"title":"üìà AI and ML in finance","type":"book"},{"authors":null,"categories":null,"content":" Table of Contents About category Posts in this category About category This section covers various topics on Recurrent Neural Networks architectures and their usage.\nWhile Convolutional networks (CNNs) seem to be very intuitive, some people have a hard time understanding the LSTM, GRU, or attention mechanisms. Posts in this category will try to explain at least some of the more complex concepts.\nPosts in this category Fifty Shades of Time: Time series \u0026amp; RNNs Time series processing is a well-known and extensively researched problem. Moreover - many people naturally understand, what a time series is, and how it is structured. Things start to get complicated once you are about to prepare time-dependent data for modeling with neural networks. Three-dimensional structures are not that easy to imagine initially.\n","date":1621123200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1621123200,"objectID":"d624f3ee0dbc3e3a523ca1a50e1aa281","permalink":"https://maddataanalyst.github.io/en/post/rnns/","publishdate":"2021-05-16T00:00:00Z","relpermalink":"/en/post/rnns/","section":"post","summary":"Section about various Recurrent Neural Network variants.","tags":null,"title":"üìä RNNs","type":"book"},{"authors":null,"categories":null,"content":"Project intro I am pleased to announce the release of software project ‚ÄúDeep Hybrid Recommenders‚Äù (DHR), accompanying the paper ‚ÄúImprovement of e-commerce recommendation systems with deep hybrid collaborative filtering with content: A case study‚Äù written by Micha≈Ç G√≥rnik and myself. This project provides Pytorch and PytorchLightning implementations of the discussed algorithms, including Deep Collaborative Filtering (DCF), Collaborative Filtering (CF), and the proposed approach - Deep Hybrid Collaborative Filtering with Content (DHCF).\nDHR also includes a repeatable experimentation environment, built using Kedro, which allows for easy experimentation and reproduction of the results from the paper. The experiment was conducted on the 2018 Amazon Reviews Dataset, and evaluated using mean squared error (MSE), mean absolute error (MAE), and mean absolute percentage error (MAPE) metrics.\nThe project will be helpful for researchers and practitioners working on product recommendation systems and data scientists seeking existing implementations of recommendation engines based on deep learning. The code is open-source and available on our GitHub repository, and we welcome any feedback or contributions from the community.\nKedro processing pipeline Next steps The future work on Deep Hybrid Recommender will focus primarily on including the graph neural networks as recommendation models, as their usability and applicability to the e-commerce domain have been proven. GNNs for heterogeneous domains (graphs with varied node types) are a relatively new and extensively researched topic with promising results. We will also be working on extending the experimentation environment to include more datasets and models.\n","date":1680825600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1680825600,"objectID":"7de7ee5edf00135a3351dc6cb4015c88","permalink":"https://maddataanalyst.github.io/en/project/deep_learning_recommenders/","publishdate":"2023-04-07T00:00:00Z","relpermalink":"/en/project/deep_learning_recommenders/","section":"project","summary":"A series of presentations, publications and implementations of deep learning algorithms as recommendation engines.","tags":["Deep Learning","recommendation systems"],"title":"Deep Learning usage as recommendation engine","type":"project"},{"authors":["Filip W√≥jcik"],"categories":null,"content":" ","date":1680381401,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1680381401,"objectID":"3489af091e88da3eb668c394e350f7c9","permalink":"https://maddataanalyst.github.io/en/publication/utilization_of_drl_for_management/","publishdate":"2023-04-01T21:36:41+01:00","relpermalink":"/en/publication/utilization_of_drl_for_management/","section":"publication","summary":"This paper tests the applicability of deep reinforcement learning (DRL) algorithms to simulated problems of constrained discrete and online resource allocation in project management","tags":["deep learning","reinforcement learning"],"title":"Utilization of deep reinforcement learning for discrete resource allocation problem in project management ‚Äì a simulation experiment","type":"publication"},{"authors":["Filip W√≥jcik"],"categories":["Tutorials","Time series","Finance"],"content":" Pixabay Intro Portfolio optimization is a critical tool for modern investors willing to maximize their capital utilization in the market and expected returns.\nMany portfolio optimization techniques are based on different variants of constrained quadratic programming formalizations, taking as inputs expected asset returns, forms of risk metrics (like covariance matrices), and transaction costs. Next, the goal is selected (typically a maximization of return while keeping the risk level constant or minimizing the risk with varying returns ). The problem is solved either via a closed-form equation or a gradient-minimizing approach. Additional elements or steps can be included, like periodical rebalancing, the inclusion of transaction costs, or different cost functions (Sharpe ratio or similar).\nThis blog post will show you how to prepare and perform a simple portfolio optimization process on selected stocks. Two approaches will be utilized - purely mathematical formulation of the optimization problem and PyPorfolioOpt library that automates most of the process.\nAfter reading this article, you will know the following:\nKey terms like portfolio, portfolio weights, and Markovitz model. How to prepare stock price data for portfolio optimization task using yfinance library. How to formulate the problem in terms of the optimization task. How to solve the optimization problem using the library cvxpy and dedicated PyPortfolioOpt. The executable notebook for this article can ba found on my Github\nPortfolio The first important thing is to define what a portfolio is. In layperson‚Äôs terms, the portfolio is a set of different assets (stocks, bonds, etc. - in general: different financial instruments) (Luenberger, 1997). Typically investors seek diversification of capital, so they want to have various types of assets.\nPortfolio optimization is finding optimal asset weights so that the portfolio return (final return after some investment period) is maximized in terms of the selected function.\nFinancial time series Data prep The portfolio optimization process starts with stock prices. You need to download historical prices for a specific period and instruments you are interested in. For this purpose, we will utilize a yahoo finance (yf) library, an unofficial tool for downloading prices from Yahoo! Finance\nLibrary link\n1 2 3 4 5 6 7 8 # Step 1: let\u0026#39;s pick some well-known stocks stocks = [\u0026#39;AAPL\u0026#39;, \u0026#39;GOOG\u0026#39;, \u0026#39;MSFT\u0026#39;] start_date = \u0026#39;2020-01-01\u0026#39; # Step 2: download prices from Yahoo! all_prices = yf.download(stocks, start=start_date) all_prices.head(3) Adj Close Close High Low Open Volume AAPL GOOG MSFT AAPL GOOG MSFT AAPL GOOG MSFT AAPL GOOG MSFT AAPL GOOG MSFT AAPL GOOG MSFT Date 2020-01-02 73.562 68.368 156.592 75.088 68.368 160.62 75.150 68.407 160.73 73.798 67.077 158.33 74.060 67.077 158.78 135480400 28132000 22622100 2020-01-03 72.846 68.033 154.642 74.357 68.033 158.62 75.145 68.625 159.95 74.125 67.277 158.06 74.287 67.393 158.32 146322800 23728000 21116200 2020-01-06 73.427 69.711 155.042 74.950 69.711 159.03 74.990 69.825 159.10 73.188 67.500 156.51 73.448 67.500 157.08 118387200 34646000 20813700 For the sake of this exercise, we will utilize only ‚ÄúClose‚Äù prices for each asset.\n1 2 close_prices = all_prices[\u0026#39;Close\u0026#39;] close_prices.head(3) AAPL GOOG MSFT Date 2020-01-02 75.088 68.368 160.62 2020-01-03 74.357 68.033 158.62 2020-01-06 74.950 69.711 159.03 Now we have a matrix of real numbers of shape T x A,\nwhere T - is the number of time steps and A is the number of assets in the portfolio.\nReturns Raw prices cannot be used for portfolio optimization. One needs to calculate the returns - price changes. There are two main forms of returns calculation (Dees \u0026amp; Sidier, 2019):\nGross returns - the price at time t, divided by the price at time t-1. $$ \\begin{align} R_t \u0026amp;\\doteq \\frac{p_t}{p_{t-1}} \\in \\mathbb{R} \\tag{1a} \\\\\\ \\end{align} $$ Simple returns - percentage change of price between t-1 and t. $$ \\begin{align} r_t \u0026amp;\\doteq \\frac{p_t - p_{t-1}}{p_{t-1}} = \\frac{p_t}{p_{t-1}} - 1 = R_t - 1 \\in \\mathbb{R} \u0026amp; \\tag{1b} \\\\\\ \\end{align} $$ Log returns - a log of simple returns used to avoid computational underflows. $$\\begin{align} p_t \u0026amp;\\doteq \\ln(R_t) \\in \\mathbb{R} \u0026amp; \\tag{1c} \\\\\\ \\end{align} $$ Here we will utilize simple returns (1a), as they are easily interpretable 1 returns = close_prices.pct_change().dropna() AAPL GOOG MSFT Date 2020-01-02 0.022816 0.022700 0.018516 2020-01-03 -0.009722 -0.004907 -0.012452 2020-01-06 0.007968 0.024657 0.002585 Portfolio optimization Porttfolio weights In this context, a portfolio is defined as a set of asset weights vector such that (Kolm et al., 2014):\n$$ \\begin{align} \\bf{\\omega} = \\left[\\omega_1, \\omega_2, \\dots, \\omega_A \\right]^T,\\quad \\sum_{i=1}^A \\omega_i = 1 \\tag{2} \\end{align} $$\nwhere each $\\omega_i$ is a weight of the i-th asset in the portfolio. One might have, e.g., 20% of Apple stocks, 40% of Google Stocks, and 40% of Microsoft stocks. Therefore [0.2, 0.4, 0.2] is a set of portfolio ‚Ä¶","date":1668038400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668038400,"objectID":"ebd21ada0396c19e4537d9b580a8d37b","permalink":"https://maddataanalyst.github.io/en/post/ml-finanace/portfolio-optimization/","publishdate":"2022-11-10T00:00:00Z","relpermalink":"/en/post/ml-finanace/portfolio-optimization/","section":"post","summary":"Portfolio optimization is a critical tool for modern investors willing to maximize their capital utilization in the market and expected returns. Many portfolio optimization techniques are based on different variants of constrained quadratic programming formalizations, taking as inputs expected asset returns, forms of risk metrics (like covariance matrices), and transaction costs.","tags":["Finance","Time-series"],"title":"Portfolio optimization","type":"book"},{"authors":["Filip W√≥jcik"],"categories":null,"content":"Reinforcement Learning (RL) is typically presented using examples of playing different games like Atari classics or physics-oriented simulators. Often one can hear opinions that RL usage in other industries and areas is very limited or challenging to implement. Such a statement is far from true - RL can be employed for similar problems as classic optimization tools, utilized by operations research (OR) for many years: production planning, scheduling, and resource optimization. With just a few mental tricks, an analyst can turn most optimization problems into a form that can be solved using these algorithms. During a presentation, a couple of such examples will be shared, as well as tips\u0026amp;tricks for designing your cases.\n","date":1655812800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655812800,"objectID":"6e0ca2c7d7f7e8ad3de55d35810c14fb","permalink":"https://maddataanalyst.github.io/en/talk/everything-is-a-game-use-cases-for-reinforcement-learning/","publishdate":"2022-06-21T12:00:00Z","relpermalink":"/en/talk/everything-is-a-game-use-cases-for-reinforcement-learning/","section":"event","summary":"With just a few mental tricks, an analyst can turn most optimization problems into a form that can be solved using these algorithms.","tags":["deep learning","reinforcement learning"],"title":"Everything is a game: use cases for Reinforcement Learning","type":"event"},{"authors":null,"categories":null,"content":"A series of presentations, publications and implementations on Reinforcement Learning algorithms.\nMain site of the project and associated experiments: https://github.com/maddataanalyst/relex\n","date":1655769600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655769600,"objectID":"177e26a68911d426d1e0801ed6671187","permalink":"https://maddataanalyst.github.io/en/project/relex/","publishdate":"2022-06-21T00:00:00Z","relpermalink":"/en/project/relex/","section":"project","summary":"A series of presentations, publications and implementations on Reinforcement Learning algorithms.","tags":["Deep Learning","reinforcement learning"],"title":"ReLeX - Reinforcement Learning Experiments","type":"project"},{"authors":["Filip W√≥jcik"],"categories":["Tutorials","Time series","RNN"],"content":"Part 1: univariate time series + classification Image by Gerd Altmann from Pixabay Intro Time series processing is a well-known and extensively researched problem. Moreover - many people naturally understand, what a time series is, and how it is structured. Things start to get complicated once you are about to prepare time-dependent data for modeling with neural networks. Three-dimensional structures are not that easy to imagine initially. After reading this blog post, you will:\nUnderstand the data should be pre-processed for LSTM neural networks; Understand the benefits of keeping time-dependent structures in data; See a practical example of wrongly and correctly prepared data for LSTM. Technologies used in this post are: Pandas Numpy Tensorflow/Keras. The code can be found: here.\nUnivariate ts Univariate time-series data is nothing more than a vector of scalar (single values) observations recorded sequentially [NIST/SEMATECH e-Handbook of Statistical Methods]. In typical use-case scenarios (like stock price modeling or evaluating some processes happening in real life), the vector is indexed by time: days, minutes, seconds, etc‚Ä¶ An example is presented below - a stock prices data, indexed by time and presented on a chart:\ntime value 2020-10-01 00:00:00 0.968858 2020-10-02 00:00:00 0.19665 2020-10-03 00:00:00 0.516261 2020-10-04 00:00:00 0.646792 2020-10-05 00:00:00 0.291559 2020-10-06 00:00:00 0.580785 2020-10-07 00:00:00 0.181799 2020-10-08 00:00:00 0.153987 2020-10-09 00:00:00 0.976227 2020-10-10 00:00:00 0.0599845 Libraries like Pandas have many functionalities related to time-series processing. Most of them are self-explanatory: you‚Äôre working with a flat vector of numbers. A detailed tutorial and explanations on this topic can be found in the official Pandas documentation.\nNeural networks \u0026amp; ts data Time-series data becomes problematic when we want to use it with neural networks. Of course, our goal is to keep the time-dependence, as we might believe, that it carries some important information.\nA family of neural networks (NN) architecture that is naturally designed to consume that kind of data is called ‚ÄúRecurrent Neural Networks‚Äù (RNN), nowadays mainly represented by Long-Short Term Memory (LSTM) or Gated-Recurrent Unit (GRU) NNs*.\n*We won‚Äôt discuss Transformers and Attention layers here, as they are players in their own league :)\nIn this post, we will discuss a classification task, so operation called: ‚Äúmany-to-one‚Äù. In this scenario, LSTM:\nTakes a time-series data as an input; Passes it step-by-step through the network; Predicts a given class. flowchart LR\rlstmT --\u0026gt; y[y pred]\rsubgraph t[LSTM T0, T1, T2... T]\rstate[state 0] --\u0026gt;LSTM1\rLSTM1 --\u0026gt;LSTM2\rLSTM2 --\u0026gt;LSTM3\rLSTM3 --\u0026gt;lstm[LSTM...]\rlstm[LSTM...]--\u0026gt; lstmT[LSTM T]\rend\rx1 --\u0026gt; LSTM1\rx2 --\u0026gt; LSTM2\rx3 --\u0026gt; LSTM3\rx[x...] --\u0026gt; lstm\rxT --\u0026gt; lstmT You can read more (and see some interesting visualizations) on the Stanford University CS230 Recurrent Neural Networks Course website [Stanford CS 230 - Recurrent Neural Networks ].\nNotation Before we proceed, let‚Äôs introduce some notation. It will be much easier to operate on symbols instead of describing everything in words :)\n$$ \\begin{align*} N -\u0026amp; \\text{ number of time windows (samples) to consider} \u0026amp; \\\\\\ y_i^t -\u0026amp; \\text{ observation in the moment t, that belongs to ith sample} \u0026amp; \\\\\\ T -\u0026amp; \\text{ maximal time-window size} \u0026amp; \\\\\\ h -\u0026amp; \\text{ LSTM units} \u0026amp; \\\\\\ c -\u0026amp; \\text{ number of classes} \u0026amp; \\\\\\ f -\u0026amp; \\text{ number of features (unique time series) to use} \\end{align*} $$\nIn this tutorial, we will operate on univariate time series, so f=1 every time\nData structures LSTMs require a particular data format that preserves a time-dependence and keeps data at least three-dimensional. What a typical LSTM network need is the following shape:\n$$ N \\times T \\times F $$\nSo a three-dimensional structure: no. of samples x time steps x features (=1 in this tutorial).\nYou might wonder - ‚Äúwhat are these samples? How do we get samples from a time series? How do I know how many steps I should have?‚Äù. Let‚Äôs first discuss this problem theoretically and then - use some concrete, practical examples.\nChoosing shape How do we decide on the number of time steps T in our structure? Most of the time, it should be an ‚Äúinformed guess‚Äù. Number T will determine how large a piece of a time series the LSTM will receive in one go. In other words - how far-seeing will it be. The decision should be based on either:\nThe properties of the time series - seasonality, autocorrelation, trends, etc. How large is the period needed to capture some relevant info? You can obtain such information by classic time-series decomposition using statistical analysis. The business rationale behind the process - e.g., preexisting knowledge about the most informative length of the sequence (the typical length of the business process or seasonality of an event). Previous studies or research presenting the optimal window size for a similar problem (e.g., time exposure to some ‚Ä¶","date":1652659200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652659200,"objectID":"df24a3a4b2a9a02f57ca56366ac109dc","permalink":"https://maddataanalyst.github.io/en/post/rnns/fifty-shapes-of-time/","publishdate":"2022-05-16T00:00:00Z","relpermalink":"/en/post/rnns/fifty-shapes-of-time/","section":"post","summary":"Time series processing is a well-known and extensively researched problem. Moreover - many people naturally understand, what a time series is, and how it is structured. Things start to get complicated once you are about to prepare time-dependent data for modeling with neural networks. Three-dimensional structures are not that easy to imagine initially.","tags":["Deep Learning","Time-series","RNN"],"title":"Fifty Shades of Time: Time series \u0026 RNNs","type":"book"},{"authors":["Filip W√≥jcik"],"categories":null,"content":"","date":1614631001,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614631001,"objectID":"57b2a5189d93ba30ce00c54bc904dae9","permalink":"https://maddataanalyst.github.io/en/publication/beyond_a_barrier_of_mistrust/","publishdate":"2021-03-01T21:36:41+01:00","relpermalink":"/en/publication/beyond_a_barrier_of_mistrust/","section":"publication","summary":"Presentation of modern explainable artificial intelligence approaches.","tags":["deep learning","xai"],"title":"Beyond the barrier of mistrust ‚Äì an overview of selected methods for explaining predictions of machine learning models","type":"publication"},{"authors":["Filip W√≥jcik"],"categories":null,"content":"","date":1614631001,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614631001,"objectID":"32abb917ee6895acf66a9acec07d76fe","permalink":"https://maddataanalyst.github.io/en/publication/deep_learning_as_recommenders/","publishdate":"2021-03-01T21:36:41+01:00","relpermalink":"/en/publication/deep_learning_as_recommenders/","section":"publication","summary":"Presentation of modern architectures of deep learning algorithms as recommendation engines","tags":["deep learning","recommendation systems"],"title":"Usage of deep neural networks as recommendation engines. Review of selected approaches","type":"publication"},{"authors":["Filip W√≥jcik"],"categories":null,"content":"The goal of this presentation and associated paper is to present results of investigation related to use of the Extreme Gradient Boosting XGBoost algorithm as a forecasting tool. The data provided by the Rossman Com-pany, with a request to design an innovative prediction method, has been used as a base for this case study. The data contains details about micro- and macro-environment, as well as turnover of 1115 stores. Performance of the algorithm was compared to classical forecasting models SARIMAX and Holt-Winters, using time-series cross validation and tests for statistical importance in prediction quality dif-ferences. Metrics of root mean squared percentage error (RMSPE), Theil‚Äôs coeffi-cient and adjusted correlation coefficient were analyzed. Results where then passed to Rossman for verification on a separate validation set, via Kaggle.com platform. Study results confirmed, that XGBoost, after using proper data preparation and training method, achieves better results than classical models.\n","date":1592481600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592481600,"objectID":"30dcaef84e30b907fd8b696aef5cad0d","permalink":"https://maddataanalyst.github.io/en/talk/one-concept-to-rule-them-all-uses-of-neural-embeddings-beyond-natural-language-processing/","publishdate":"2020-06-18T12:00:00Z","relpermalink":"/en/talk/one-concept-to-rule-them-all-uses-of-neural-embeddings-beyond-natural-language-processing/","section":"event","summary":"Neural embeddings are one of the most popular techniques used in Natural Language Processing to represent word similarities. There are many variations and implementations of this concept - starting from skip-gram, through GloVe or Word2Vec. But embeddings are a much more powerful concept that can be utilized in many different areas, not only NLP. Possible applications of embeddings in recommendation engines, item similarity detection, and market basket analysis through frequent items search will be presented during this speech.","tags":["Deep Learning","recommendation systems"],"title":"One concept to rule them all - uses of neural embeddings beyond natural language processing","type":"event"},{"authors":["Filip W√≥jcik","Micha≈Ç G√≥rnik"],"categories":null,"content":"","date":1583267801,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583267801,"objectID":"202bc887224d3c04096d105fa7d1432c","permalink":"https://maddataanalyst.github.io/en/publication/deep_hybrid_recommendation_system/","publishdate":"2020-03-03T21:36:41+01:00","relpermalink":"/en/publication/deep_hybrid_recommendation_system/","section":"publication","summary":"A presentation of new algorithm for recommendations with hybrid deep learning architecture","tags":["deep learning","recommendation systems"],"title":"Improvement of e-commerce recommendation systems with deep hybrid collaborative filtering with content: A case study","type":"publication"},{"authors":["Filip W√≥jcik"],"categories":null,"content":"Recommendation engines are probably the most important tools for modern e-commerce organizations. As the data volumes grow larger and larger (as well as product/user base), traditional approaches based on collaborative might be not enough. During this talk I will present alternative approaches to recommendations - making use of deep stacked autoencoders.\n","date":1560513600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560513600,"objectID":"b89daa788f5be9edfbcc9dd933dd10d7","permalink":"https://maddataanalyst.github.io/en/talk/recommendation-engines-based-on-autoencoders/","publishdate":"2019-06-14T12:00:00Z","relpermalink":"/en/talk/recommendation-engines-based-on-autoencoders/","section":"event","summary":"How autoencoders can be used as recommendation engines, replacing collaborative filtering or other approaches.","tags":["deep learning","recommendation systems"],"title":"Recommendation engines based on autoencoders","type":"event"},{"authors":["Filip W√≥jcik"],"categories":null,"content":"The goal of this presentation and associated paper is to present results of investigation related to use of the Extreme Gradient Boosting XGBoost algorithm as a forecasting tool. The data provided by the Rossman Com-pany, with a request to design an innovative prediction method, has been used as a base for this case study. The data contains details about micro- and macro-environment, as well as turnover of 1115 stores. Performance of the algorithm was compared to classical forecasting models SARIMAX and Holt-Winters, using time-series cross validation and tests for statistical importance in prediction quality dif-ferences. Metrics of root mean squared percentage error (RMSPE), Theil‚Äôs coeffi-cient and adjusted correlation coefficient were analyzed. Results where then passed to Rossman for verification on a separate validation set, via Kaggle.com platform. Study results confirmed, that XGBoost, after using proper data preparation and training method, achieves better results than classical models.\n","date":1528459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528459200,"objectID":"c2f97d35cd05ba5ee4a11cb75d04f3c1","permalink":"https://maddataanalyst.github.io/en/talk/xgboost-as-a-time-series-forecasting-tool/","publishdate":"2018-06-08T12:00:00Z","relpermalink":"/en/talk/xgboost-as-a-time-series-forecasting-tool/","section":"event","summary":"can we use XGBoost as a time-series forecasting tool?","tags":["machine learning","forecasting"],"title":"XGBoost as a time-series forecasting tool","type":"event"},{"authors":null,"categories":null,"content":"A series of publications, conference talks and other materials, related to the usage of XGBoost as a financial forecasting tool.\n","date":1528416000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528416000,"objectID":"d678f39e2a9e48ee90698fa0aeb81683","permalink":"https://maddataanalyst.github.io/en/project/xgboost_forecast/","publishdate":"2018-06-08T00:00:00Z","relpermalink":"/en/project/xgboost_forecast/","section":"project","summary":"A series of presentations and publications about time-series forecasting with XGBoost tool","tags":["forecasting","xgboost"],"title":"Forecasting with XGboost","type":"project"},{"authors":["Filip W√≥jcik"],"categories":null,"content":"Human capital and human resources are key success factors for multiple knowledge-based companies. Managing competences of employees is one of the most important areas for modern management science. This paper presents, an innovative algorithm based on natural language processing (NLP) and association analysis for recognition, assignment and evaluation of competences in a knowledge-rich organizations. An algorithm performs keywords extraction from applicant‚Äôs and employees resumes, which are used later on in a supervised phase. Achieved results - 71% of balanced accuracy in a case study suggest, that a system can recognize important correlations between competences and assigned projects.\n","date":1524052800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524052800,"objectID":"b9a13e1e1497a2742d6d3ca59cb051dd","permalink":"https://maddataanalyst.github.io/en/talk/machine-learning-and-artificial-intelligence-as-a-decision-support-systems-for-human-capital-management/","publishdate":"2018-04-18T12:00:00Z","relpermalink":"/en/talk/machine-learning-and-artificial-intelligence-as-a-decision-support-systems-for-human-capital-management/","section":"event","summary":"How can machine learning help in managing human resources in organizations?","tags":["machine learning"],"title":"Machine learning and artificial intelligence as a decision support systems for human capital management","type":"event"},{"authors":["Filip W√≥jcik"],"categories":null,"content":"","date":1519862400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519862400,"objectID":"d59f0b066df46317beaeb89b7cf3bbc6","permalink":"https://maddataanalyst.github.io/en/publication/xgboost_turnover/","publishdate":"2018-03-01T00:00:00Z","relpermalink":"/en/publication/xgboost_turnover/","section":"publication","summary":"Analysis of XGBoost applications to financial forecasting and sales management.","tags":["xgboost","forecasting","econometrics"],"title":"Forecasting Daily Turnover Using Xgboost Algorithm ‚Äì A Case Study","type":"publication"},{"authors":["dr hab. Iwona Chomiak-Orsa, prof. UE","Filip W√≥jcik"],"categories":null,"content":"","date":1493596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493596800,"objectID":"3eaa1b65b4ee3d9467461f7d934fc250","permalink":"https://maddataanalyst.github.io/en/publication/business_relevant_attributes_2017/","publishdate":"2017-05-01T00:00:00Z","relpermalink":"/en/publication/business_relevant_attributes_2017/","section":"publication","summary":"Modern decision support systems make use of machine learning and artificial intelligence to solve complicated problems. One of them is classification, understood in this context as assigning objects to categories. Amongst many methods to achieve this goal, rulebased systems pay special attention, because they provide an end-user not only with direct answers to a given problem, but also produce useful insights into correlations present in a dataset. In this article new method has been proposed ‚àí application and modification of Leo Breiman‚Äôs original Random Forest solution combined with backwards elimination (known from classic regression) ‚àí and tested on real credit decisions dataset. Differences in classification metrics between base and augmented classifier were checked using cross-validation testing, and statistical significance. The article concludes with further research suggestions.","tags":["machine learning","operations research"],"title":"Detecting business-relevant attributes in rule-based classification","type":"publication"},{"authors":["Filip W√≥jcik"],"categories":null,"content":"H2O AI is one of the most interesting out-of-the-box machine learning tools. It has plenty of algorithms implemented, those algorithms are really fast and effective and it integrates well with R and Spark. One of them is especially interesing - Gradient Boosting. Want to know more?\n","date":1491566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491566400,"objectID":"397adc297b22d692d52da27154ddceba","permalink":"https://maddataanalyst.github.io/en/talk/water-powered-machine-learning-gradient-boosting-machines-in-h2o-ai/","publishdate":"2017-04-07T12:00:00Z","relpermalink":"/en/talk/water-powered-machine-learning-gradient-boosting-machines-in-h2o-ai/","section":"event","summary":"H2O AI is one of the most interesting out-of-the-box machine learning tools. It has plenty of algorithms implemented, those algorithms are really fast and effective and it integrates well with R and Spark. One of them is especially interesing - Gradient Boosting. Want to know more?","tags":["machine learning"],"title":"Water powered machine learning: gradient boosting machines in H2O AI","type":"event"},{"authors":["Filip W√≥jcik"],"categories":null,"content":"Machine learning algorithms are replacement for an old-fashioned advisory systems. How we can utilize them in such a way? How white-box systems are different from black-box?\n","date":1461844800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461844800,"objectID":"674b63d3f34cffb89b7b41221f980905","permalink":"https://maddataanalyst.github.io/en/talk/decision-support-systems-remade-machine-learning-advisers/","publishdate":"2016-04-28T12:00:00Z","relpermalink":"/en/talk/decision-support-systems-remade-machine-learning-advisers/","section":"event","summary":"Machine learning algorithms are replacement for an old-fashioned advisory systems. How we can utilize them in such a way? How white-box systems are different from black-box?","tags":["machine learning"],"title":"Decision support systems remade: (machine) learning advisers","type":"event"},{"authors":["Filip W√≥jcik"],"categories":null,"content":"","date":1446724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1446724800,"objectID":"1d0e54b5eb038a4f81b3fc4b7974d926","permalink":"https://maddataanalyst.github.io/en/talk/machine-learning-when-big-data-is-not-enough/","publishdate":"2015-11-05T12:00:00Z","relpermalink":"/en/talk/machine-learning-when-big-data-is-not-enough/","section":"event","summary":"Presentation of various machine learning capabilities, as a tools to make sense out of large datasets","tags":["machine learning"],"title":"Machine learning - when Big Data is not enough","type":"event"}]