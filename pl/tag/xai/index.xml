<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>xai | NeuraMind</title>
    <link>https://maddataanalyst.github.io/pl/tag/xai/</link>
      <atom:link href="https://maddataanalyst.github.io/pl/tag/xai/index.xml" rel="self" type="application/rss+xml" />
    <description>xai</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>pl</language><lastBuildDate>Mon, 01 Mar 2021 21:36:41 +0100</lastBuildDate>
    <image>
      <url>https://maddataanalyst.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>xai</title>
      <link>https://maddataanalyst.github.io/pl/tag/xai/</link>
    </image>
    
    <item>
      <title>Przełamać nieufność – metody zapewniania przejrzystości modeli uczenia maszynowego w zastosowaniach ekonomicznych</title>
      <link>https://maddataanalyst.github.io/pl/publication/beyond_a_barrier_of_mistrust/</link>
      <pubDate>Mon, 01 Mar 2021 21:36:41 +0100</pubDate>
      <guid>https://maddataanalyst.github.io/pl/publication/beyond_a_barrier_of_mistrust/</guid>
      <description>&lt;p&gt;Wraz z upowszechnieniem się stosowania uczenia maszynowego, jako zestawu technik i narzędzi do rozwiązywania problemów biznesowych, zaczął narastać problem zapewnienia ich interpretowalności i budowania zaufania do generowanych predykcji [Gunning i in., 2019]. Zjawisko to nasiliło się, po osiągnięciu (w niektórych dziedzinach) przez głębokie sieci neuronowe wyników, przekraczających możliwości człowieka, przy jednoczesnym braku prostych metod ustalenia źródeł podejmowanych przez nie decyzji [Holzinger, 2018]. Wejście w życie przepisów nakładających obowiązek ścisłej ochrony danych osobowych i zapewnienia transparentności procesu ich przetwarzania (RODO) stały się przyczynkiem do dyskusji nad koncepcją XAI – „możliwych do wyjaśnienia metod sztucznej inteligencji” (ang. eXplainable Artificial Intelligence) [Holzinger i in., 2018]. Obszary życia społecznego, o szczególnie istotnym znaczeniu i wrażliwości (jak np. medycyna, wymiar sprawiedliwości czy rynki finansowe), w których wykorzystywane jest uczenie maszynowe, powinny mieć dostęp do technik pogłębionej analizy otrzymywanych wyników. Niektórzy autorzy postulują nawet, by zaprzestać używania algorytmów o wyjątkowo skomplikowanej wewnętrznej strukturze, w przypadku decyzji niosących ze sobą wysokie koszty potencjalnej pomyłki [Rudin, 2019].&lt;/p&gt;
&lt;p&gt;Podczas wystąpienia zaprezentowane zostaną nowoczesne koncepcje i metody, mające na celu budowanie większego zaufania do inteligentnych systemów komputerowych, poprzez przedstawianie ich interpretacji, czy objaśnień. Przybliżone zostaną m.in. SHAP (ang. shapley additive explanations), LIME (ang. locally interpretable model-agnostic explanations), Anchor i metoda granicy decyzyjnej (ang. decision boundary) [Hase &amp;amp; Bansal, 2020].&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Literatura:&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Gunning, D., Stefik, M., Choi, J., Miller, T., Stumpf, S., &amp;amp; Yang, G. Z. (2019). XAI-Explainable artificial intelligence. Science Robotics.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Hase, P., &amp;amp; Bansal, M. (2020). Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior? arXiv preprint arXiv:2005.01831.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Holzinger, A. (2018). From machine learning to explainable AI. DISA 2018 - IEEE World Symposium on Digital Intelligence for Systems and Machines, Proceedings, January, 55–66.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Holzinger, A., Kieseberg, P., Weippl, E., &amp;amp; Tjoa, A. M. (2018). Current advances, trends and challenges of machine learning and knowledge extraction: from machine learning to explainable AI. International Cross-Domain Conference for Machine Learning and Knowledge Extraction, 1–8.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Rudin, C. (2019). Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 1(5), 206–215.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
  </channel>
</rss>
